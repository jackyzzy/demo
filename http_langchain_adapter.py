"""
HTTPæ¨¡å‹çš„LangChainé€‚é…å™¨
å°†HTTPæ¨¡å‹å®¢æˆ·ç«¯é€‚é…ä¸ºLangChainå…¼å®¹çš„èŠå¤©æ¨¡å‹
"""

from typing import Any, Dict, List, Optional, Iterator
from pydantic import Field
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.outputs import ChatResult, ChatGeneration
from langchain_core.callbacks import CallbackManagerForLLMRun

from http_model_client import HttpModelClient, create_http_client, HttpModelResponse


class HttpChatModel(BaseChatModel):
    """HTTPæ¨¡å‹çš„LangChainèŠå¤©æ¨¡å‹é€‚é…å™¨"""
    
    # Pydanticå­—æ®µå®šä¹‰
    url: str = Field(description="APIç«¯ç‚¹URL")
    api_key: str = Field(description="APIå¯†é’¥")
    model_id: str = Field(description="æ¨¡å‹ID") 
    vendor: str = Field(default="generic", description="ä¾›åº”å•†åç§°")
    headers: Optional[Dict[str, str]] = Field(default=None, description="é¢å¤–çš„HTTPå¤´éƒ¨")
    temperature: float = Field(default=0.7, description="æ¸©åº¦å‚æ•°")
    max_tokens: Optional[int] = Field(default=None, description="æœ€å¤§ä»¤ç‰Œæ•°")
    timeout: int = Field(default=60, description="è¯·æ±‚è¶…æ—¶æ—¶é—´")
    max_retries: int = Field(default=3, description="æœ€å¤§é‡è¯•æ¬¡æ•°")
    client: Optional[HttpModelClient] = Field(default=None, exclude=True, description="HTTPå®¢æˆ·ç«¯")
    
    def __init__(self,
                 url: str,
                 api_key: str,
                 model_id: str,
                 vendor: str = "generic",
                 headers: Optional[Dict[str, str]] = None,
                 temperature: float = 0.7,
                 max_tokens: Optional[int] = None,
                 timeout: int = 60,
                 max_retries: int = 3,
                 **kwargs):
        """
        åˆå§‹åŒ–HTTPèŠå¤©æ¨¡å‹
        
        Args:
            url: APIç«¯ç‚¹URL
            api_key: APIå¯†é’¥
            model_id: æ¨¡å‹ID
            vendor: ä¾›åº”å•†åç§°
            headers: é¢å¤–çš„HTTPå¤´éƒ¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            **kwargs: å…¶ä»–å‚æ•°
        """
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œä¼ é€’æ‰€æœ‰å‚æ•°
        super().__init__(
            url=url,
            api_key=api_key,
            model_id=model_id,
            vendor=vendor,
            headers=headers,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
            max_retries=max_retries,
            **kwargs
        )
        
        # åˆ›å»ºHTTPå®¢æˆ·ç«¯
        self.client = create_http_client(
            url=url,
            api_key=api_key,
            model_id=model_id,
            vendor=vendor,
            headers=headers,
            timeout=timeout,
            max_retries=max_retries
        )
    
    def _convert_messages_to_http(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:
        """
        å°†LangChainæ¶ˆæ¯è½¬æ¢ä¸ºHTTP APIæ ¼å¼
        
        Args:
            messages: LangChainæ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            HTTP APIæ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        """
        http_messages = []
        
        for message in messages:
            if isinstance(message, HumanMessage):
                role = "user"
            elif isinstance(message, AIMessage):
                role = "assistant"
            elif isinstance(message, SystemMessage):
                role = "system"
            else:
                # é»˜è®¤ä½œä¸ºç”¨æˆ·æ¶ˆæ¯å¤„ç†
                role = "user"
            
            http_messages.append({
                "role": role,
                "content": message.content
            })
        
        return http_messages
    
    def _convert_http_response_to_langchain(self, response: HttpModelResponse) -> ChatResult:
        """
        å°†HTTPå“åº”è½¬æ¢ä¸ºLangChainæ ¼å¼
        
        Args:
            response: HTTPæ¨¡å‹å“åº”
            
        Returns:
            LangChainèŠå¤©ç»“æœ
        """
        message = AIMessage(content=response.content)
        
        generation = ChatGeneration(
            message=message,
            generation_info={
                "finish_reason": response.finish_reason,
                "model": response.model,
                "usage": response.usage
            }
        )
        
        return ChatResult(generations=[generation])
    
    def _generate(self,
                  messages: List[BaseMessage],
                  stop: Optional[List[str]] = None,
                  run_manager: Optional[CallbackManagerForLLMRun] = None,
                  **kwargs: Any) -> ChatResult:
        """
        ç”ŸæˆèŠå¤©å“åº”
        
        Args:
            messages: è¾“å…¥æ¶ˆæ¯
            stop: åœæ­¢åºåˆ—
            run_manager: å›è°ƒç®¡ç†å™¨
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            èŠå¤©ç»“æœ
        """
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        http_messages = self._convert_messages_to_http(messages)
        
        # å‡†å¤‡å‚æ•°
        temperature = kwargs.get("temperature", self.temperature)
        max_tokens = kwargs.get("max_tokens", self.max_tokens)
        
        # è°ƒç”¨HTTPå®¢æˆ·ç«¯
        try:
            response = self.client.chat_completion(
                messages=http_messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
            
            # è½¬æ¢å“åº”
            return self._convert_http_response_to_langchain(response)
            
        except Exception as e:
            # å¦‚æœå‡ºé”™ï¼Œè¿”å›é”™è¯¯æ¶ˆæ¯
            error_message = AIMessage(content=f"HTTPæ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")
            generation = ChatGeneration(message=error_message)
            return ChatResult(generations=[generation])
    
    def _stream(self,
                messages: List[BaseMessage],
                stop: Optional[List[str]] = None,
                run_manager: Optional[CallbackManagerForLLMRun] = None,
                **kwargs: Any) -> Iterator[ChatGeneration]:
        """
        æµå¼ç”ŸæˆèŠå¤©å“åº”
        
        Args:
            messages: è¾“å…¥æ¶ˆæ¯
            stop: åœæ­¢åºåˆ—
            run_manager: å›è°ƒç®¡ç†å™¨
            **kwargs: é¢å¤–å‚æ•°
            
        Yields:
            èŠå¤©ç”Ÿæˆç‰‡æ®µ
        """
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        http_messages = self._convert_messages_to_http(messages)
        
        # å‡†å¤‡å‚æ•°
        temperature = kwargs.get("temperature", self.temperature)
        max_tokens = kwargs.get("max_tokens", self.max_tokens)
        
        try:
            # è°ƒç”¨æµå¼HTTPå®¢æˆ·ç«¯
            for chunk in self.client.stream_chat_completion(
                messages=http_messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            ):
                message = AIMessage(content=chunk.content)
                yield ChatGeneration(message=message)
                
        except Exception as e:
            # å¦‚æœå‡ºé”™ï¼Œç”Ÿæˆé”™è¯¯æ¶ˆæ¯
            error_message = AIMessage(content=f"HTTPæµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            yield ChatGeneration(message=error_message)
    
    @property
    def _llm_type(self) -> str:
        """è¿”å›LLMç±»å‹"""
        return f"http_{self.vendor}"
    
    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """è¿”å›è¯†åˆ«å‚æ•°"""
        return {
            "model_id": self.model_id,
            "url": self.url,
            "vendor": self.vendor,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }


def create_http_chat_model(url: str,
                          api_key: str,
                          model_id: str,
                          vendor: str = "generic",
                          **kwargs) -> HttpChatModel:
    """
    åˆ›å»ºHTTPèŠå¤©æ¨¡å‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        url: APIç«¯ç‚¹URL
        api_key: APIå¯†é’¥
        model_id: æ¨¡å‹ID
        vendor: ä¾›åº”å•†åç§°
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        HTTPèŠå¤©æ¨¡å‹å®ä¾‹
    """
    return HttpChatModel(
        url=url,
        api_key=api_key,
        model_id=model_id,
        vendor=vendor,
        **kwargs
    )


if __name__ == "__main__":
    # æµ‹è¯•HTTP LangChainé€‚é…å™¨
    print("ğŸ§ª æµ‹è¯•HTTP LangChainé€‚é…å™¨")
    
    # ç¤ºä¾‹é…ç½®
    test_config = {
        "url": "https://api.example.com/v1/chat/completions",
        "api_key": "test-key",
        "model_id": "test-model",
        "vendor": "generic"
    }
    
    try:
        # åˆ›å»ºHTTPèŠå¤©æ¨¡å‹
        http_model = create_http_chat_model(**test_config)
        print(f"âœ… æˆåŠŸåˆ›å»ºHTTP LangChainæ¨¡å‹")
        print(f"ğŸ·ï¸  æ¨¡å‹ç±»å‹: {http_model._llm_type}")
        print(f"ğŸ†” è¯†åˆ«å‚æ•°: {http_model._identifying_params}")
        
        # æµ‹è¯•æ¶ˆæ¯è½¬æ¢
        from langchain_core.messages import HumanMessage, SystemMessage
        
        test_messages = [
            SystemMessage(content="You are a helpful assistant."),
            HumanMessage(content="Hello!")
        ]
        
        http_messages = http_model._convert_messages_to_http(test_messages)
        print(f"ğŸ“ è½¬æ¢åçš„æ¶ˆæ¯: {http_messages}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")