Total samples: 40, batch size: 16, total batches: 3
INFO 11-01 11:59:07 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/mnt/public_data/Qwen/Qwen2.5-7B-Instruct_main', speculative_config=None, tokenizer='/mnt/public_data/Qwen/Qwen2.5-7B-Instruct_main', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=17000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/mnt/public_data/Qwen/Qwen2.5-7B-Instruct_main)
INFO 11-01 11:59:15 model_runner.py:146] Loading model weights took 14.2487 GB
INFO 11-01 11:59:17 gpu_executor.py:83] # GPU blocks: 3724, # CPU blocks: 4681
INFO 11-01 11:59:19 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-01 11:59:19 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-01 11:59:22 model_runner.py:924] Graph capturing finished in 3 secs.
INFO 11-01 11:59:22 block_manager_v1.py:247] Automatic prefix caching is enabled.

Running batch inference:   0%|                           | 0/3 [00:00<?, ?it/s]

Processed prompts:   0%| | 0/16 [00:00<?, ?it/s, Generation Speed: 0.00 toks/s][A
Running batch inference:   0%|                           | 0/3 [00:01<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/jovyan/exam/problems_ai_infra/question1/test_1_opt2.py", line 85, in <module>
[rank0]:     outputs = llm.generate(batch, sampling_params)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/vllm/utils.py", line 672, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 300, in generate
[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 552, in _run_engine
[rank0]:     step_outputs = self.llm_engine.step()
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 772, in step
[rank0]:     output = self.model_executor.execute_model(
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/vllm/executor/gpu_executor.py", line 91, in execute_model
[rank0]:     output = self.driver_worker.execute_model(execute_model_req)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/vllm/worker/worker.py", line 272, in execute_model
[rank0]:     output = self.model_runner.execute_model(seq_group_metadata_list,
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 738, in execute_model
[rank0]:     output = self.model.sample(
[rank0]:              ^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py", line 345, in sample
[rank0]:     next_tokens = self.sampler(logits, sampling_metadata)
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py", line 96, in forward
[rank0]:     sample_results, maybe_sampled_tokens_tensor = _sample(
[rank0]:                                                   ^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py", line 655, in _sample
[rank0]:     return _sample_with_torch(
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py", line 542, in _sample_with_torch
[rank0]:     sample_results = _greedy_sample(seq_groups, greedy_samples)
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py", line 288, in _greedy_sample
[rank0]:     samples = samples.tolist()
[rank0]:               ^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: CUDA error: an illegal memory access was encountered
[rank0]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank0]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


Processed prompts:   0%| | 0/16 [00:01<?, ?it/s, Generation Speed: 0.00 toks/s]
