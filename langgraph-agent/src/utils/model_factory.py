"""
LangGraphæ¨¡å‹å·¥å‚
ç”¨äºåˆ›å»ºä¸åŒæä¾›å•†çš„æ¨¡å‹å®ä¾‹
"""

from typing import Any, Optional
import os

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../../'))

from model_config import ModelManager, ModelConfig, ModelProvider

# HTTPæ¨¡å‹é€‚é…å™¨
try:
    sys.path.append(os.path.join(os.path.dirname(__file__), '../../../'))
    from http_langchain_adapter import create_http_chat_model
    HTTP_ADAPTER_AVAILABLE = True
except ImportError:
    HTTP_ADAPTER_AVAILABLE = False
    print("âš ï¸ HTTPæ¨¡å‹é€‚é…å™¨ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ä¾èµ–")


class LangGraphModelFactory:
    """LangGraphæ¡†æ¶æ¨¡å‹å·¥å‚"""
    
    @staticmethod
    def create_model(model_key: str, **kwargs) -> Any:
        """
        åˆ›å»ºæ¨¡å‹å®ä¾‹
        
        Args:
            model_key: æ¨¡å‹é…ç½®é”®å€¼
            **kwargs: é¢å¤–çš„æ¨¡å‹å‚æ•°
            
        Returns:
            æ¨¡å‹å®ä¾‹
        """
        config = ModelManager.get_model_config(model_key)
        if not config:
            raise ValueError(f"æœªæ‰¾åˆ°æ¨¡å‹é…ç½®: {model_key}")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
        if not ModelManager.is_model_available(model_key):
            raise ValueError(f"æ¨¡å‹ {model_key} ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®")
        
        # åˆå¹¶é…ç½®å‚æ•°
        model_params = {
            'model': config.model_id,
            'temperature': config.temperature,
            'max_tokens': config.max_tokens,
            **kwargs  # å…è®¸è¦†ç›–é»˜è®¤å‚æ•°
        }
        
        # æ ¹æ®æä¾›å•†åˆ›å»ºç›¸åº”çš„æ¨¡å‹å®ä¾‹
        if config.provider == ModelProvider.OPENAI:
            return LangGraphModelFactory._create_openai_model(config, model_params)
        
        elif config.provider == ModelProvider.ANTHROPIC:
            return LangGraphModelFactory._create_anthropic_model(config, model_params)
        
        elif config.provider == ModelProvider.GROQ:
            return LangGraphModelFactory._create_groq_model(config, model_params)
        
        elif config.provider == ModelProvider.OLLAMA:
            return LangGraphModelFactory._create_ollama_model(config, model_params)
        
        elif config.provider == ModelProvider.DEEPSEEK:
            return LangGraphModelFactory._create_deepseek_model(config, model_params)
        
        elif config.provider == ModelProvider.HTTP:
            return LangGraphModelFactory._create_http_model(config, model_params)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {config.provider}")
    
    @staticmethod
    def _create_openai_model(config: ModelConfig, params: dict) -> ChatOpenAI:
        """åˆ›å»ºOpenAIæ¨¡å‹"""
        # ä½¿ç”¨é…ç½®ä¸­çš„APIå¯†é’¥
        if config.api_key:
            params['api_key'] = config.api_key
        
        return ChatOpenAI(**params)
    
    @staticmethod
    def _create_anthropic_model(config: ModelConfig, params: dict) -> ChatAnthropic:
        """åˆ›å»ºAnthropicæ¨¡å‹"""
        # ä½¿ç”¨é…ç½®ä¸­çš„APIå¯†é’¥
        if config.api_key:
            params['anthropic_api_key'] = config.api_key
        
        return ChatAnthropic(**params)
    
    @staticmethod
    def _create_groq_model(config: ModelConfig, params: dict) -> ChatOpenAI:
        """åˆ›å»ºGroqæ¨¡å‹ï¼ˆé€šè¿‡OpenAIå…¼å®¹æ¥å£ï¼‰"""
        # Groqä½¿ç”¨OpenAIå…¼å®¹çš„æ¥å£
        groq_params = params.copy()
        groq_params['base_url'] = "https://api.groq.com/openai/v1"
        
        # ä½¿ç”¨é…ç½®ä¸­çš„APIå¯†é’¥
        if config.api_key:
            groq_params['api_key'] = config.api_key
        
        return ChatOpenAI(**groq_params)
    
    @staticmethod
    def _create_ollama_model(config: ModelConfig, params: dict) -> ChatOpenAI:
        """åˆ›å»ºOllamaæœ¬åœ°æ¨¡å‹"""
        # Ollamaé€šè¿‡OpenAIå…¼å®¹æ¥å£
        ollama_params = params.copy()
        ollama_params['base_url'] = config.base_url or "http://localhost:11434/v1"
        ollama_params['api_key'] = "ollama"  # Ollamaä¸éªŒè¯APIå¯†é’¥ï¼Œä½†éœ€è¦æä¾›
        
        return ChatOpenAI(**ollama_params)
    
    @staticmethod
    def _create_deepseek_model(config: ModelConfig, params: dict) -> ChatOpenAI:
        """åˆ›å»ºDeepSeekæ¨¡å‹"""
        # DeepSeekä½¿ç”¨OpenAIå…¼å®¹çš„æ¥å£
        deepseek_params = params.copy()
        deepseek_params['base_url'] = config.base_url or "https://api.deepseek.com"
        
        # ä½¿ç”¨é…ç½®ä¸­çš„APIå¯†é’¥
        if config.api_key:
            deepseek_params['api_key'] = config.api_key
        
        # å¯¹äºè‡ªå®šä¹‰éƒ¨ç½²çš„DeepSeek-R1ï¼Œåˆ›å»ºå…¼å®¹çš„ChatOpenAIå®ä¾‹
        if "custom" in config.model_id.lower() or (config.base_url and "modelarts" in config.base_url):
            # ç§»é™¤å¯èƒ½ä¸æ”¯æŒçš„å‚æ•°
            deepseek_params.pop('max_completion_tokens', None)
            
            # åˆ›å»ºChatOpenAIå®ä¾‹å¹¶ç¦ç”¨max_completion_tokens
            chat_model = ChatOpenAI(**deepseek_params)
            
            # é‡å†™_get_request_payloadæ–¹æ³•æ¥é˜»æ­¢max_completion_tokensçš„ä½¿ç”¨
            original_get_request_payload = chat_model._get_request_payload
            
            def custom_get_request_payload(input_, **kwargs):
                payload = original_get_request_payload(input_, **kwargs)
                # ç§»é™¤max_completion_tokensï¼Œä½¿ç”¨max_tokens
                if 'max_completion_tokens' in payload:
                    max_val = payload.pop('max_completion_tokens')
                    payload['max_tokens'] = max_val
                return payload
            
            chat_model._get_request_payload = custom_get_request_payload
            return chat_model
        
        return ChatOpenAI(**deepseek_params)
    
    @staticmethod
    def _create_http_model(config: ModelConfig, params: dict) -> Any:
        """åˆ›å»ºHTTPæ¨¡å‹"""
        if not HTTP_ADAPTER_AVAILABLE:
            raise ValueError("HTTPæ¨¡å‹é€‚é…å™¨ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ä¾èµ–å®‰è£…")
        
        if not config.base_url:
            raise ValueError(f"HTTPæ¨¡å‹ {config.name} ç¼ºå°‘base_urlé…ç½®")
        
        if not config.api_key:
            raise ValueError(f"HTTPæ¨¡å‹ {config.name} çš„APIå¯†é’¥æœªé…ç½®: {config.api_key_env}")
        
        # æå–å‚æ•°
        http_params = {
            'url': config.base_url,
            'api_key': config.api_key,
            'model_id': config.model_id,
            'vendor': config.vendor or "generic",
            'temperature': params.get('temperature', config.temperature),
            'max_tokens': params.get('max_tokens', config.max_tokens)
        }
        
        # æ·»åŠ è‡ªå®šä¹‰å¤´éƒ¨
        if config.headers:
            http_params['headers'] = config.headers
        
        import logging
        logging.info(f"ğŸŒ åˆ›å»ºHTTPæ¨¡å‹: {config.name} (ä¾›åº”å•†: {config.vendor})")
        logging.info(f"ğŸ“¡ ç«¯ç‚¹: {config.base_url}")
        
        # åˆ›å»ºHTTPèŠå¤©æ¨¡å‹
        return create_http_chat_model(**http_params)
    
    @staticmethod
    def list_supported_models() -> list:
        """è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        return ModelManager.list_available_models()
    
    @staticmethod
    def get_available_models() -> dict:
        """è·å–å½“å‰å¯ç”¨çš„æ¨¡å‹"""
        return ModelManager.get_models_by_availability()


# ä¾¿æ·å‡½æ•°
def create_langgraph_model(model_key: str = "gpt-4o", **kwargs) -> Any:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºLangGraphæ¨¡å‹å®ä¾‹
    
    Args:
        model_key: æ¨¡å‹é”®å€¼ï¼Œé»˜è®¤gpt-4o
        **kwargs: é¢å¤–å‚æ•°
    
    Returns:
        æ¨¡å‹å®ä¾‹
    """
    return LangGraphModelFactory.create_model(model_key, **kwargs)


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹å·¥å‚
    print("ğŸ§ª æµ‹è¯•LangGraphæ¨¡å‹å·¥å‚")
    
    # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
    available = LangGraphModelFactory.get_available_models()
    print(f"å¯ç”¨æ¨¡å‹æ•°é‡: {len(available)}")
    
    for key, config in available.items():
        print(f"- {key}: {config.name}")
    
    # æµ‹è¯•åˆ›å»ºæ¨¡å‹ï¼ˆé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹ï¼‰
    if available:
        test_model_key = list(available.keys())[0]
        print(f"\nğŸ”§ æµ‹è¯•åˆ›å»ºæ¨¡å‹: {test_model_key}")
        
        try:
            model = create_langgraph_model(test_model_key)
            print(f"âœ… æˆåŠŸåˆ›å»ºæ¨¡å‹: {model}")
        except Exception as e:
            print(f"âŒ åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
    else:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•")