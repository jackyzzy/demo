import os
from typing import List, Dict, Any
import json
from datetime import datetime

from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from dotenv import load_dotenv

from states.agent_state import AgentState
from tools import web_search, duckduckgo_search, get_stock_info, calculator
from utils import create_langgraph_model

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

from model_config import ModelManager, display_available_models, ModelProvider

load_dotenv()

tools = [web_search, duckduckgo_search, get_stock_info, calculator]

class LangGraphAgent:
    """åŸºäºLangGraphçš„æ™ºèƒ½åŠ©æ‰‹"""
    
    def __init__(self, model_key: str = "gpt-4o"):
        """
        åˆå§‹åŒ–æ™ºèƒ½åŠ©æ‰‹
        
        Args:
            model_key: ä½¿ç”¨çš„æ¨¡å‹é”®å€¼
        """
        self.model_key = model_key

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
        if not ModelManager.is_model_available(model_key):
            print(f"âŒ æ¨¡å‹ {model_key} ä¸å¯ç”¨")
            available_models = ModelManager.get_models_by_availability()
            if available_models:
                fallback_key = list(available_models.keys())[0]
                print(f"ğŸ”„ ä½¿ç”¨å›é€€æ¨¡å‹: {fallback_key}")
                self.model_key = fallback_key
            else:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®")
                display_available_models()
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        
        # ä½¿ç”¨æ¨¡å‹å·¥å‚åˆ›å»ºæ¨¡å‹
        try:
            # ä¸ºHTTPæ¨¡å‹è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
            model_config = ModelManager.get_model_config(self.model_key)
            if model_config and model_config.provider == ModelProvider.HTTP:
                self.llm = create_langgraph_model(self.model_key, temperature=0.1, timeout=30)
                print(f"ğŸŒ HTTPæ¨¡å‹ï¼Œè®¾ç½®è¶…æ—¶æ—¶é—´: 30ç§’")
            else:
                self.llm = create_langgraph_model(self.model_key, temperature=0.1)
        except Exception as e:
            print(f"âŒ åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            raise
            
        self.memory = MemorySaver()
        
        # åˆ›å»ºå·¥å…·èŠ‚ç‚¹
        self.tool_node = ToolNode(tools)
        
        # æ„å»ºå›¾
        self.app = self._build_graph()
        
        # è·å–æ¨¡å‹é…ç½®ä¿¡æ¯
        model_config = ModelManager.get_model_config(self.model_key)
        
        print(f"âœ… LangGraphæ™ºèƒ½åŠ©æ‰‹åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ ä½¿ç”¨æ¨¡å‹: {self.model_key}")
        if model_config:
            print(f"ğŸ·ï¸  æ¨¡å‹åç§°: {model_config.name}")
            print(f"ğŸ¢ æä¾›å•†: {model_config.provider.value}")
    
    def _build_graph(self) -> StateGraph:
        """æ„å»ºLangGraphå·¥ä½œæµå›¾"""
        
        # åˆ›å»ºçŠ¶æ€å›¾
        workflow = StateGraph(AgentState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("classifier", self._classify_task)          
        workflow.add_node("simple_chat", self._simple_chat)           
        workflow.add_node("planner", self._create_plan)              
        workflow.add_node("researcher", self._research_node)          
        workflow.add_node("analyzer", self._analysis_node)           
        workflow.add_node("reasoning", self._reasoning_node)         
        workflow.add_node("tools", self.tool_node)                   
        workflow.add_node("synthesizer", self._synthesize_results)   
        
        # æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±
        workflow.add_edge(START, "classifier")
        
        # ä»åˆ†ç±»å™¨åˆ°ä¸åŒå¤„ç†è·¯å¾„
        workflow.add_conditional_edges(
            "classifier",
            self._route_after_classification,
            {
                "simple_chat": "simple_chat",
                "research": "planner", 
                "analysis": "planner",
                "planning": "planner"
            }
        )
        
        # ç®€å•å¯¹è¯è·¯å¾„
        workflow.add_conditional_edges(
            "simple_chat",
            self._should_use_tools,
            {
                "tools": "tools",
                "end": END
            }
        )
        
        # å·¥å…·è°ƒç”¨åå›åˆ°ç®€å•å¯¹è¯
        workflow.add_edge("tools", "simple_chat")
        
        # è§„åˆ’åçš„è·¯å¾„
        workflow.add_edge("planner", "researcher")
        workflow.add_edge("researcher", "analyzer")  
        workflow.add_edge("analyzer", "reasoning")
        workflow.add_edge("reasoning", "synthesizer")
        workflow.add_edge("synthesizer", END)
        
        return workflow.compile(checkpointer=self.memory)
    
    def _classify_task(self, state: AgentState) -> Dict[str, Any]:
        """ä»»åŠ¡åˆ†ç±»èŠ‚ç‚¹"""
        messages = state.get("messages", [])
        if not messages:
            return {"task_type": "simple_chat"}
            
        last_message = messages[-1].content if messages else ""
        
        # ä½¿ç”¨LLMè¿›è¡Œä»»åŠ¡åˆ†ç±»
        classification_prompt = f"""
        åˆ†æç”¨æˆ·çš„è¯·æ±‚ï¼Œå°†å…¶åˆ†ç±»ä¸ºä»¥ä¸‹ç±»å‹ä¹‹ä¸€ï¼š
        1. simple_chat - ç®€å•å¯¹è¯ã€é—®å€™ã€åŸºæœ¬é—®ç­”
        2. research - éœ€è¦æœç´¢ä¿¡æ¯çš„ç ”ç©¶ä»»åŠ¡
        3. analysis - éœ€è¦æ·±åº¦åˆ†æçš„ä»»åŠ¡
        4. planning - éœ€è¦åˆ¶å®šè®¡åˆ’æˆ–ç­–ç•¥çš„ä»»åŠ¡
        
        ç”¨æˆ·è¯·æ±‚: {last_message}
        
        åªè¿”å›ç±»å‹åç§°ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚
        """
        
        response = self.llm.invoke([HumanMessage(content=classification_prompt)])
        task_type = response.content.strip().lower()
        
        # éªŒè¯è¿”å›å€¼
        valid_types = ["simple_chat", "research", "analysis", "planning"]
        if task_type not in valid_types:
            task_type = "simple_chat"
        
        return {
            "task_type": task_type,
            "current_task": last_message,
            "reasoning_steps": [f"ä»»åŠ¡åˆ†ç±»: {task_type}"]
        }
    
    def _simple_chat(self, state: AgentState) -> Dict[str, Any]:
        """ç®€å•å¯¹è¯å¤„ç†èŠ‚ç‚¹"""
        messages = state.get("messages", [])
        
        # æ„å»ºå¯¹è¯æç¤º
        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€æœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚ä½ å¯ä»¥ï¼š
        1. è¿›è¡Œæ—¥å¸¸å¯¹è¯
        2. å›ç­”ä¸€èˆ¬æ€§é—®é¢˜
        3. åœ¨éœ€è¦æ—¶è°ƒç”¨å·¥å…·è·å–ä¿¡æ¯
        4. è¿›è¡Œæ•°å­¦è®¡ç®—
        5. æœç´¢æœ€æ–°ä¿¡æ¯
        
        å¦‚æœç”¨æˆ·çš„é—®é¢˜éœ€è¦æœç´¢æœ€æ–°ä¿¡æ¯ã€è‚¡ç¥¨æ•°æ®æˆ–æ•°å­¦è®¡ç®—ï¼Œè¯·è°ƒç”¨ç›¸åº”çš„å·¥å…·ã€‚
        """
        
        # ä¿æŒLangGraphçš„æ¶ˆæ¯æ ¼å¼
        # æ„å»ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼Œä¿æŒLangGraphæ¶ˆæ¯æ ¼å¼
        system_message = SystemMessage(content=system_prompt)
        full_messages = [system_message] + messages
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯DeepSeekæ¨¡å‹æˆ–HTTPæ¨¡å‹ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ä½¿ç”¨ä¸åŒçš„å¤„ç†æ–¹å¼
        model_config = ModelManager.get_model_config(self.model_key)
        
        if model_config and (model_config.provider.value == "deepseek" or model_config.provider == ModelProvider.HTTP):
            # DeepSeekæ¨¡å‹å’ŒHTTPæ¨¡å‹ç›´æ¥å¤„ç†ï¼Œä¸ä½¿ç”¨å¤æ‚çš„å·¥å…·è°ƒç”¨
            response = self.llm.invoke(full_messages)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
            content = response.content.lower()
            if any(keyword in content for keyword in ["è®¡ç®—", "æœç´¢", "è‚¡ç¥¨", "æ•°å­¦"]):
                # å°è¯•æå–å¹¶æ‰§è¡Œè®¡ç®—
                if "è®¡ç®—" in content or "æ•°å­¦" in content:
                    try:
                        # æå–æ•°å­¦è¡¨è¾¾å¼
                        import re
                        user_message = messages[-1].content if messages else ""
                        
                        # æ›´å¼ºå¤§çš„æ•°å­¦è¡¨è¾¾å¼æå–
                        math_patterns = [
                            r'\([\d\s+\-*/]+\)\s*[\+\-\*/]\s*\d+',  # (125 + 75) * 2
                            r'\d+\s*[\+\-\*/]\s*\d+[\s\+\-\*/\d]*',  # 125 + 75 * 2
                            r'\([\d\s+\-*/]+\)',  # (125 + 75)
                        ]
                        
                        for pattern in math_patterns:
                            math_match = re.search(pattern, user_message)
                            if math_match:
                                expression = math_match.group(0).strip()
                                # å¦‚æœæ‰¾åˆ°å®Œæ•´è¡¨è¾¾å¼ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨æ•´ä¸ªæ•°å­¦éƒ¨åˆ†
                                if not expression:
                                    continue
                                
                                # å¯¹äºé¢˜ç›®"(125 + 75) * 2 - 50"ï¼Œæå–å®Œæ•´è¡¨è¾¾å¼
                                full_match = re.search(r'([\d\s+\-*/\(\)]+)', user_message)
                                if full_match:
                                    full_expr = full_match.group(0).strip()
                                    if len(full_expr) > len(expression):
                                        expression = full_expr
                                
                                calc_result = calculator.invoke({"expression": expression})
                                response.content = f"{response.content}\n\n{calc_result}"
                                break
                    except:
                        pass
        else:
            # å…¶ä»–æ¨¡å‹ä½¿ç”¨æ ‡å‡†çš„å·¥å…·ç»‘å®š
            response = self.llm.bind_tools(tools).invoke(full_messages)
        
        return {
            "messages": messages + [response]
        }
    
    def _create_plan(self, state: AgentState) -> Dict[str, Any]:
        """ä»»åŠ¡è§„åˆ’èŠ‚ç‚¹"""
        current_task = state.get("current_task", "")
        task_type = state.get("task_type", "")
        
        planning_prompt = f"""
        ä¸ºä»¥ä¸‹{task_type}ä»»åŠ¡åˆ¶å®šè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’ï¼š
        
        ä»»åŠ¡: {current_task}
        
        è¯·å°†ä»»åŠ¡åˆ†è§£ä¸ºå…·ä½“çš„æ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤åŒ…æ‹¬ï¼š
        1. æ­¥éª¤åç§°
        2. å…·ä½“è¡ŒåŠ¨
        3. é¢„æœŸç»“æœ
        
        ä»¥JSONæ ¼å¼è¿”å›è®¡åˆ’ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        {{
            "steps": [
                {{
                    "name": "æ­¥éª¤åç§°",
                    "action": "å…·ä½“è¡ŒåŠ¨", 
                    "expected_result": "é¢„æœŸç»“æœ"
                }}
            ]
        }}
        """
        
        response = self.llm.invoke([HumanMessage(content=planning_prompt)])
        
        try:
            plan_data = json.loads(response.content)
            plan = plan_data.get("steps", [])
        except:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆ›å»ºé»˜è®¤è®¡åˆ’
            plan = [
                {
                    "name": "ä¿¡æ¯æ”¶é›†",
                    "action": "æœç´¢ç›¸å…³ä¿¡æ¯",
                    "expected_result": "è·å¾—èƒŒæ™¯ä¿¡æ¯"
                },
                {
                    "name": "æ·±åº¦åˆ†æ", 
                    "action": "åˆ†ææ”¶é›†åˆ°çš„ä¿¡æ¯",
                    "expected_result": "å½¢æˆåˆæ­¥ç»“è®º"
                },
                {
                    "name": "æ¨ç†ç»¼åˆ",
                    "action": "è¿›è¡Œé€»è¾‘æ¨ç†",
                    "expected_result": "å¾—å‡ºæœ€ç»ˆç­”æ¡ˆ"
                }
            ]
        
        reasoning_steps = state.get("reasoning_steps", [])
        reasoning_steps.append(f"åˆ¶å®šäº†åŒ…å«{len(plan)}ä¸ªæ­¥éª¤çš„æ‰§è¡Œè®¡åˆ’")
        
        return {
            "plan": plan,
            "reasoning_steps": reasoning_steps
        }
    
    def _research_node(self, state: AgentState) -> Dict[str, Any]:
        """ç ”ç©¶ä¿¡æ¯æ”¶é›†èŠ‚ç‚¹"""
        current_task = state.get("current_task", "")
        
        # æ‰§è¡Œç ”ç©¶æ­¥éª¤
        search_queries = self._generate_search_queries(current_task)
        search_results = []
        
        for query in search_queries[:3]:  
            try:
                # ä½¿ç”¨Tavilyæœç´¢
                result = web_search.invoke({"query": query, "max_results": 3})
                search_results.append({
                    "query": query,
                    "results": result
                })
            except:
                # å¤‡ç”¨DuckDuckGoæœç´¢
                try:
                    result = duckduckgo_search.invoke({"query": query, "max_results": 3})
                    search_results.append({
                        "query": query, 
                        "results": result
                    })
                except:
                    pass
        
        reasoning_steps = state.get("reasoning_steps", [])
        reasoning_steps.append(f"å®Œæˆä¿¡æ¯æ”¶é›†ï¼Œè·å¾—{len(search_results)}ç»„æœç´¢ç»“æœ")
        
        return {
            "search_results": search_results,
            "reasoning_steps": reasoning_steps
        }
    
    def _analysis_node(self, state: AgentState) -> Dict[str, Any]:
        """åˆ†æå¤„ç†èŠ‚ç‚¹"""
        search_results = state.get("search_results", [])
        current_task = state.get("current_task", "")
        
        # æ•´åˆæœç´¢ç»“æœ
        all_info = ""
        for result in search_results:
            all_info += f"æŸ¥è¯¢: {result['query']}\nç»“æœ: {result['results']}\n\n"
        
        analysis_prompt = f"""
        åŸºäºæ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œå¯¹ä»¥ä¸‹ä»»åŠ¡è¿›è¡Œæ·±åº¦åˆ†æï¼š
        
        ä»»åŠ¡: {current_task}
        
        æ”¶é›†åˆ°çš„ä¿¡æ¯:
        {all_info}
        
        è¯·è¿›è¡Œå¤šè§’åº¦åˆ†æï¼š
        1. å…³é”®ä¿¡æ¯æ€»ç»“
        2. é‡è¦å‘ç°å’Œè¶‹åŠ¿
        3. ä¸åŒè§‚ç‚¹å¯¹æ¯”
        4. æ½œåœ¨å½±å“å’Œæ„ä¹‰
        
        è¯·ä»¥ç»“æ„åŒ–æ–¹å¼ç»„ç»‡åˆ†æç»“æœã€‚
        """
        
        response = self.llm.invoke([HumanMessage(content=analysis_prompt)])
        
        analysis_results = {
            "summary": "åˆ†æå®Œæˆ",
            "content": response.content,
            "timestamp": datetime.now().isoformat()
        }
        
        reasoning_steps = state.get("reasoning_steps", [])
        reasoning_steps.append("å®Œæˆä¿¡æ¯åˆ†æï¼Œå½¢æˆç»“æ„åŒ–è§è§£")
        
        return {
            "analysis_results": analysis_results,
            "reasoning_steps": reasoning_steps
        }
    
    def _reasoning_node(self, state: AgentState) -> Dict[str, Any]:
        """æ¨ç†æ€è€ƒèŠ‚ç‚¹"""
        analysis_results = state.get("analysis_results", {})
        current_task = state.get("current_task", "")
        
        reasoning_prompt = f"""
        åŸºäºåˆ†æç»“æœï¼Œè¯·è¿›è¡Œé€æ­¥æ¨ç†æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š
        
        åŸå§‹é—®é¢˜: {current_task}
        åˆ†æç»“æœ: {analysis_results.get('content', '')}
        
        è¯·ä½¿ç”¨ä»¥ä¸‹æ¨ç†æ¡†æ¶ï¼š
        
        1. é—®é¢˜ç†è§£ï¼šé‡æ–°é˜è¿°æ ¸å¿ƒé—®é¢˜
        2. å…³é”®å› ç´ ï¼šè¯†åˆ«å½±å“ç­”æ¡ˆçš„å…³é”®å› ç´   
        3. é€»è¾‘æ¨ç†ï¼š
           - æ­¥éª¤1ï¼š[åŸºäºè¯æ®Aå¾—å‡ºç»“è®º1]
           - æ­¥éª¤2ï¼š[åŸºäºè¯æ®Bå¾—å‡ºç»“è®º2]
           - æ­¥éª¤3ï¼š[ç»¼åˆç»“è®º1å’Œ2å¾—å‡ºæœ€ç»ˆç»“è®º]
        4. éªŒè¯æ£€æŸ¥ï¼šæ£€éªŒç»“è®ºçš„åˆç†æ€§
        5. æœ€ç»ˆç­”æ¡ˆï¼šæ¸…æ™°æ˜ç¡®çš„å›ç­”
        
        è¯·ä¸¥æ ¼æŒ‰ç…§è¿™ä¸ªç»“æ„è¿›è¡Œæ¨ç†ã€‚
        """
        
        response = self.llm.invoke([HumanMessage(content=reasoning_prompt)])
        
        reasoning_steps = state.get("reasoning_steps", [])
        reasoning_steps.append("å®Œæˆé€»è¾‘æ¨ç†åˆ†æ")
        
        return {
            "reasoning_steps": reasoning_steps,
            "reasoning_content": response.content
        }
    
    def _synthesize_results(self, state: AgentState) -> Dict[str, Any]:
        """ç»“æœç»¼åˆèŠ‚ç‚¹"""
        current_task = state.get("current_task", "")
        analysis_results = state.get("analysis_results", {})
        reasoning_content = state.get("reasoning_content", "")
        reasoning_steps = state.get("reasoning_steps", [])
        
        synthesis_prompt = f"""
        è¯·å°†æ‰€æœ‰åˆ†æå’Œæ¨ç†ç»“æœç»¼åˆæˆä¸€ä¸ªå®Œæ•´ã€æ¸…æ™°çš„æœ€ç»ˆå›ç­”ï¼š
        
        åŸå§‹é—®é¢˜: {current_task}
        
        åˆ†æç»“æœ: {analysis_results.get('content', '')}
        
        æ¨ç†è¿‡ç¨‹: {reasoning_content}
        
        è¯·æä¾›ï¼š
        1. ç›´æ¥æ˜ç¡®çš„ç­”æ¡ˆ
        2. æ”¯æŒè¯æ®å’Œç†ç”±
        3. ç›¸å…³çš„èƒŒæ™¯ä¿¡æ¯
        4. å¦‚æœæœ‰çš„è¯ï¼Œå»ºè®®ä¸‹ä¸€æ­¥è¡ŒåŠ¨
        
        è¯·ä»¥è‡ªç„¶ã€æ˜“æ‡‚çš„æ–¹å¼ç»„ç»‡å›ç­”ã€‚
        """
        
        final_response = self.llm.invoke([HumanMessage(content=synthesis_prompt)])
        
        reasoning_steps.append("å®Œæˆç»“æœç»¼åˆï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”")
        
        messages = state.get("messages", [])
        messages.append(AIMessage(content=final_response.content))
        
        return {
            "messages": messages,
            "reasoning_steps": reasoning_steps
        }
    
    def _generate_search_queries(self, task: str) -> List[str]:
        """ç”Ÿæˆæœç´¢æŸ¥è¯¢"""
        query_prompt = f"""
        ä¸ºä»¥ä¸‹ä»»åŠ¡ç”Ÿæˆ2-3ä¸ªæœ‰æ•ˆçš„æœç´¢æŸ¥è¯¢ï¼š
        
        ä»»åŠ¡: {task}
        
        è¯·ç”Ÿæˆèƒ½å¤Ÿè·å¾—ç›¸å…³ã€å‡†ç¡®ä¿¡æ¯çš„æœç´¢æŸ¥è¯¢ã€‚
        æ¯è¡Œä¸€ä¸ªæŸ¥è¯¢ï¼Œä¸è¦ç¼–å·ã€‚
        """
        
        response = self.llm.invoke([HumanMessage(content=query_prompt)])
        queries = [q.strip() for q in response.content.split('\n') if q.strip()]
        
        return queries[:3]
    
    def _should_use_tools(self, state: AgentState) -> str:
        """å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·"""
        messages = state.get("messages", [])
        if not messages:
            return "end"
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯DeepSeekæ¨¡å‹æˆ–HTTPæ¨¡å‹
        model_config = ModelManager.get_model_config(self.model_key)
        if model_config and (model_config.provider.value == "deepseek" or model_config.provider == ModelProvider.HTTP):
            # DeepSeekæ¨¡å‹å’ŒHTTPæ¨¡å‹ä¸ä½¿ç”¨LangGraphçš„å·¥å…·è°ƒç”¨æœºåˆ¶
            return "end"
        
        # å…¶ä»–æ¨¡å‹ä½¿ç”¨æ ‡å‡†çš„tools_condition
        return tools_condition(state)
    
    def _route_after_classification(self, state: AgentState) -> str:
        """åˆ†ç±»åçš„è·¯ç”±å†³ç­–"""
        task_type = state.get("task_type", "simple_chat")
        return task_type
    
    def chat(self, message: str, session_id: str = "default") -> str:
        """
        ä¸æ™ºèƒ½åŠ©æ‰‹å¯¹è¯
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯IDï¼Œç”¨äºç»´æŠ¤å¯¹è¯å†å²
        
        Returns:
            str: åŠ©æ‰‹å›å¤
        """
        try:
            # å‡†å¤‡åˆå§‹çŠ¶æ€
            initial_state = {
                "messages": [HumanMessage(content=message)],
                "current_task": "",
                "task_type": "",
                "search_results": [],
                "analysis_results": {},
                "reasoning_steps": [],
                "plan": [],
                "completed_steps": [],
                "context": {},
                "next_action": ""
            }
            
            # æ‰§è¡Œå›¾å¤„ç†
            config = {"configurable": {"thread_id": session_id}}
            final_state = self.app.invoke(initial_state, config=config)
            
            # æå–å›å¤
            messages = final_state.get("messages", [])
            if messages and isinstance(messages[-1], AIMessage):
                return messages[-1].content
            else:
                return "æŠ±æ­‰ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°äº†é—®é¢˜ã€‚"
                
        except Exception as e:
            error_msg = f"å¯¹è¯å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ é”™è¯¯: {error_msg}")
            return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def get_reasoning_steps(self, session_id: str = "default") -> List[str]:
        """è·å–æ¨ç†æ­¥éª¤"""
        try:
            config = {"configurable": {"thread_id": session_id}}
            state = self.app.get_state(config)
            return state.values.get("reasoning_steps", [])
        except:
            return []
    
    def switch_model(self, new_model_key: str):
        """åˆ‡æ¢æ¨¡å‹"""
        if ModelManager.is_model_available(new_model_key):
            try:
                # åˆ›å»ºæ–°æ¨¡å‹
                new_llm = create_langgraph_model(new_model_key, temperature=0.1)
                
                # æ›´æ–°æ¨¡å‹
                self.model_key = new_model_key
                self.llm = new_llm
                
                # é‡æ–°æ„å»ºå›¾
                self.app = self._build_graph()
                
                model_config = ModelManager.get_model_config(new_model_key)
                print(f"âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹: {new_model_key}")
                if model_config:
                    print(f"ğŸ·ï¸  æ¨¡å‹åç§°: {model_config.name}")
                    print(f"ğŸ¢ æä¾›å•†: {model_config.provider.value}")
            except Exception as e:
                print(f"âŒ åˆ‡æ¢æ¨¡å‹å¤±è´¥: {e}")
        else:
            print(f"âŒ æ¨¡å‹ {new_model_key} ä¸å¯ç”¨")
            print("ğŸ¤– å¯ç”¨æ¨¡å‹:")
            available_models = ModelManager.get_models_by_availability()
            for key, config in available_models.items():
                print(f"  - {key}: {config.name}")
    
    def list_available_models(self):
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        display_available_models()